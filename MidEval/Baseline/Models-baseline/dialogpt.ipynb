{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11118100,"sourceType":"datasetVersion","datasetId":6932700}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:17:53.219517Z","iopub.execute_input":"2025-03-22T10:17:53.219762Z","iopub.status.idle":"2025-03-22T10:17:54.140208Z","shell.execute_reply.started":"2025-03-22T10:17:53.219740Z","shell.execute_reply":"2025-03-22T10:17:54.139353Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/intentconan2/validation.csv\n/kaggle/input/intentconan2/train.csv\n/kaggle/input/intentconan2/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,DataCollatorForSeq2Seq\nfrom torch.utils.data import Dataset\nimport random\nfrom datasets import Dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:17:54.141245Z","iopub.execute_input":"2025-03-22T10:17:54.141784Z","iopub.status.idle":"2025-03-22T10:18:15.946052Z","shell.execute_reply.started":"2025-03-22T10:17:54.141747Z","shell.execute_reply":"2025-03-22T10:18:15.945377Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_path = \"/kaggle/input/intentconan2/train.csv\"   \ntest_path = \"/kaggle/input/intentconan2/test.csv\"     \nval_path = \"/kaggle/input/intentconan2/validation.csv\"\n\ndf_train = pd.read_csv(train_path) \ndf_test = pd.read_csv(test_path)\ndf_val = pd.read_csv(val_path)\n\nprint(\"Train Data Size:\", df_train.shape)\nprint(\"Test Data Size:\", df_test.shape)\nprint(\"Val data size : \", df_val.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:18:15.947007Z","iopub.execute_input":"2025-03-22T10:18:15.947752Z","iopub.status.idle":"2025-03-22T10:18:16.763104Z","shell.execute_reply.started":"2025-03-22T10:18:15.947708Z","shell.execute_reply":"2025-03-22T10:18:16.762163Z"}},"outputs":[{"name":"stdout","text":"Train Data Size: (9532, 31)\nTest Data Size: (2971, 32)\nVal data size :  (1470, 32)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"intent_tokens = {\n    'positive': '<positive>',\n    'informative': '<informative>',\n    'questioning': '<questioning>',\n    'denouncing': '<denouncing>'\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:18:16.765687Z","iopub.execute_input":"2025-03-22T10:18:16.765937Z","iopub.status.idle":"2025-03-22T10:18:16.769464Z","shell.execute_reply.started":"2025-03-22T10:18:16.765916Z","shell.execute_reply":"2025-03-22T10:18:16.768721Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"microsoft/DialoGPT-medium\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nspecial_tokens_dict = {'additional_special_tokens': list(intent_tokens.values())}\ntokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:18:16.771003Z","iopub.execute_input":"2025-03-22T10:18:16.771235Z","iopub.status.idle":"2025-03-22T10:18:29.539296Z","shell.execute_reply.started":"2025-03-22T10:18:16.771216Z","shell.execute_reply":"2025-03-22T10:18:29.538540Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d0be7f5b7740cab4ab87a5f05aef71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05145b22655843e58624556a2ab930ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9996b7e3da7147479adfce5826adf0f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19804b9bb3894cdb874225689fd48739"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/863M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76a5471d1404a059268937fb30160a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6851b086a354654bb48343853fa8024"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Embedding(50261, 1024)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def format_function(example):\n    intent_token = intent_tokens.get(example[\"csType\"].strip().lower(), \"<informative>\")\n    text = example[\"hatespeech\"] + tokenizer.eos_token + intent_token + \" \" + example[\"counterspeech\"] + tokenizer.eos_token\n    return {\"text\": text}\n\ntrain_dataset = Dataset.from_pandas(df_train)\nval_dataset = Dataset.from_pandas(df_val)\n\ntrain_dataset = train_dataset.map(format_function)\nval_dataset = val_dataset.map(format_function)\n\n# Step 6: Tokenize\ndef tokenize_function(example):\n    encodings = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=256\n    )\n    encodings[\"labels\"] = encodings[\"input_ids\"].copy()  \n    return encodings\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:18:29.540267Z","iopub.execute_input":"2025-03-22T10:18:29.540603Z","iopub.status.idle":"2025-03-22T10:18:36.508071Z","shell.execute_reply.started":"2025-03-22T10:18:29.540576Z","shell.execute_reply":"2025-03-22T10:18:36.507247Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9532 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d1e95a714846928ac4d59356187e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1470 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d58a220255c745aebc3b28c1fe728008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9532 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e28a15300c9455f84525ced4dc3e3b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1470 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"865d41956f294bbda2123b12dcc85158"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:18:36.508855Z","iopub.execute_input":"2025-03-22T10:18:36.509051Z","iopub.status.idle":"2025-03-22T10:18:36.512680Z","shell.execute_reply.started":"2025-03-22T10:18:36.509033Z","shell.execute_reply":"2025-03-22T10:18:36.511778Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./dialogpt_intent_cs\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    warmup_steps=100,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=50,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\"\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T10:18:36.513578Z","iopub.execute_input":"2025-03-22T10:18:36.513865Z","iopub.status.idle":"2025-03-22T11:29:20.589698Z","shell.execute_reply.started":"2025-03-22T10:18:36.513836Z","shell.execute_reply":"2025-03-22T11:29:20.588738Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14298' max='14298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14298/14298 1:10:38, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.406300</td>\n      <td>0.414662</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.303200</td>\n      <td>0.432024</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.253800</td>\n      <td>0.443795</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=14298, training_loss=0.3645724961433832, metrics={'train_runtime': 4239.4017, 'train_samples_per_second': 6.745, 'train_steps_per_second': 3.373, 'total_flos': 1.3278562530361344e+16, 'train_loss': 0.3645724961433832, 'epoch': 3.0})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Example: Generate counterspeech with intent\ninput_hs = \"Go back to your country\"\nintent = \"<denouncing>\"\n\ninput_text = input_hs + tokenizer.eos_token + intent + \" \"\ninput_ids = tokenizer.encode(input_text, return_tensors='pt')\ninput_ids = input_ids.to(model.device)\n\noutput = model.generate(\n    input_ids,\n    max_length=100,\n    pad_token_id=tokenizer.eos_token_id,\n    do_sample=True,\n    top_k=50,\n    top_p=0.95,\n    temperature=0.9,\n    num_return_sequences=1\n)\n\nreply = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(\"Generated:\", reply.replace(input_hs, \"\").strip())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T11:29:20.590693Z","iopub.execute_input":"2025-03-22T11:29:20.591019Z","iopub.status.idle":"2025-03-22T11:29:21.608246Z","shell.execute_reply.started":"2025-03-22T11:29:20.590984Z","shell.execute_reply":"2025-03-22T11:29:21.607492Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Generated: everyone deserves a chance to be treated with respect and dignity in their country, regardless of their nationality. promoting inclusivity and diversity enriches our society and fosters a better world for all.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport torch\n\n\n\n# Inference on test set\ngenerated_counterspeeches = []\n\nfor i, row in tqdm(df_test.iterrows(), total=len(df_test)):\n    hs = row['hatespeech']\n    intent = intent_tokens.get(row['csType'].strip().lower(), '<informative>')\n    \n    # Prepare input\n    input_text = hs + tokenizer.eos_token + intent + \" \"\n    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n    input_ids = input_ids.to(model.device)\n    \n    # Generate response\n    output_ids = model.generate(\n        input_ids,\n        max_length=100,\n        pad_token_id=tokenizer.eos_token_id,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.9\n    )\n    \n    # Extract only the generated part (excluding input prompt)\n    new_tokens = output_ids[0][input_ids.shape[-1]:]\n    response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n    \n    generated_counterspeeches.append(response)\n\n# Save predictions to a new DataFrame\ndf_test[\"generated_counterspeech\"] = generated_counterspeeches\n\n# Save to CSV\noutput_path = \"/kaggle/working/generated_test_predictions.csv\"\ndf_test.to_csv(output_path, index=False)\n\nprint(f\"Saved generated counterspeeches to: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T11:29:21.608983Z","iopub.execute_input":"2025-03-22T11:29:21.609186Z","iopub.status.idle":"2025-03-22T11:51:12.291411Z","shell.execute_reply.started":"2025-03-22T11:29:21.609168Z","shell.execute_reply":"2025-03-22T11:51:12.290434Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2971/2971 [21:50<00:00,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"Saved generated counterspeeches to: /kaggle/working/generated_test_predictions.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10}]}
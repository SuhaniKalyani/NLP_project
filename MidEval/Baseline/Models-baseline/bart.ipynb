{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11113264,"sourceType":"datasetVersion","datasetId":6928939}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset, DatasetDict\nfrom transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T07:34:03.187962Z","iopub.execute_input":"2025-03-21T07:34:03.188217Z","iopub.status.idle":"2025-03-21T07:34:25.757043Z","shell.execute_reply.started":"2025-03-21T07:34:03.188196Z","shell.execute_reply":"2025-03-21T07:34:25.756353Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\n# âœ… Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# âœ… Load BART model & tokenizer (Use cached model)\nMODEL_NAME = \"facebook/bart-large-cnn\"\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n\n\n# âœ… Load only first 10 rows (Faster)\ntrain_df = pd.read_csv(\"/kaggle/input/intentconan2/train.csv\")\nval_df = pd.read_csv(\"/kaggle/input/intentconan2/validation.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/intentconan2/test.csv\")\n\nprint(\"Train Dataset Sample:\\n\", train_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T07:34:25.757832Z","iopub.execute_input":"2025-03-21T07:34:25.758467Z","iopub.status.idle":"2025-03-21T07:34:43.678510Z","shell.execute_reply.started":"2025-03-21T07:34:25.758436Z","shell.execute_reply":"2025-03-21T07:34:43.677457Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34369f53870f4be28978e8ca0e6d39ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9c95676d3c46fe981509063df1d752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ae42f8317054835849e2aa62b167297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f92cf89a38149229de31581a6a34075"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044ab1baf81347c3bf331aaaa2e7933e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ed6928a71342ff94ff862e1904a770"}},"metadata":{}},{"name":"stdout","text":"Train Dataset Sample:\n                                           hatespeech       csType  \\\n0  Maybe the UN could talk to those asian and afr...  Informative   \n1  Maybe the UN could talk to those asian and afr...  Questioning   \n2  Maybe the UN could talk to those asian and afr...   Denouncing   \n3  Maybe the UN could talk to those asian and afr...     Positive   \n4  Juice Jews are worse that nukes. Caption to a ...  Informative   \n\n                                       counterspeech Suggest  Relevance  \\\n0  The us is the second most polluting country in...       3        4.0   \n1  Doesn't everyone on the planet have a responsi...                3.0   \n2  The world would be a better place if people we...       1        1.0   \n3  You're right, ocean pollution is one of the ma...       3        4.0   \n4  Anti-semitism is a serious problem that we nee...       2        3.0   \n\n   Aggressive  Complexity  Comments source  \\\n0         2.0         3.0       NaN  Human   \n1         2.0         2.0       NaN  Human   \n2         2.0         2.0       NaN  Human   \n3         2.0         2.0       NaN  Human   \n4         2.0         2.0       NaN  Human   \n\n                                               claim  ... hatespeechTarget  \\\n0  The UN should focus on Asian and African natio...  ...     asian_people   \n1  The UN should focus on Asian and African natio...  ...     asian_people   \n2  The UN should focus on Asian and African natio...  ...     asian_people   \n3  The UN should focus on Asian and African natio...  ...     asian_people   \n4                         Jews are worse than nukes.  ...             jews   \n\n  powerDynamics                               prompt_offensiveness  \\\n0        racial  Analyze the offensiveness of the statement: Ma...   \n1        racial  Analyze the offensiveness of the statement: Ma...   \n2        racial  Analyze the offensiveness of the statement: Ma...   \n3        racial  Analyze the offensiveness of the statement: Ma...   \n4      religion  Analyze the offensiveness of the statement: Ju...   \n\n                                 prompt_target_group  \\\n0  Identify the group of people that the speaker ...   \n1  Identify the group of people that the speaker ...   \n2  Identify the group of people that the speaker ...   \n3  Identify the group of people that the speaker ...   \n4  Identify the group of people that the speaker ...   \n\n                               prompt_speaker_intent  \\\n0  Analyze the speaker's intention behind writing...   \n1  Analyze the speaker's intention behind writing...   \n2  Analyze the speaker's intention behind writing...   \n3  Analyze the speaker's intention behind writing...   \n4  Analyze the speaker's intention behind writing...   \n\n                               prompt_power_dynamics  \\\n0  Explain the underlying power dynamics between ...   \n1  Explain the underlying power dynamics between ...   \n2  Explain the underlying power dynamics between ...   \n3  Explain the underlying power dynamics between ...   \n4  Explain the underlying power dynamics between ...   \n\n                                  prompt_implication  \\\n0  Explain the implied meaning underlying the off...   \n1  Explain the implied meaning underlying the off...   \n2  Explain the implied meaning underlying the off...   \n3  Explain the implied meaning underlying the off...   \n4  Explain the implied meaning underlying the off...   \n\n                           prompt_emotional_reaction  \\\n0  Describe how the target group might feel emoti...   \n1  Describe how the target group might feel emoti...   \n2  Describe how the target group might feel emoti...   \n3  Describe how the target group might feel emoti...   \n4  Describe how the target group might feel emoti...   \n\n                           prompt_cognitive_reaction  \\\n0  Describe how the target group might react cogn...   \n1  Describe how the target group might react cogn...   \n2  Describe how the target group might react cogn...   \n3  Describe how the target group might react cogn...   \n4  Describe how the target group might react cogn...   \n\n                                prompt_cs_generation  \n0  Analyze the different aspects such as offensiv...  \n1  Analyze the different aspects such as offensiv...  \n2  Analyze the different aspects such as offensiv...  \n3  Analyze the different aspects such as offensiv...  \n4  Analyze the different aspects such as offensiv...  \n\n[5 rows x 31 columns]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [f\"Hate Speech: {hs} Intent: {intent}\" for hs, intent in zip(examples[\"hatespeech\"], examples[\"csType\"])]\n    targets = examples[\"counterspeech\"]\n\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=250, truncation=True, padding=\"max_length\")\n\n    # Replace padding token ids with -100 for loss function\n    labels[\"input_ids\"] = [\n        [(token if token != tokenizer.pad_token_id else -100) for token in label] for label in labels[\"input_ids\"]\n    ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n\n# âœ… Convert Pandas to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# âœ… Tokenize dataset\ntokenized_datasets = DatasetDict({\n    \"train\": train_dataset.map(preprocess_function, batched=True),\n    \"validation\": val_dataset.map(preprocess_function, batched=True),\n    \"test\": test_dataset.map(preprocess_function, batched=True),\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T07:35:07.968675Z","iopub.execute_input":"2025-03-21T07:35:07.968982Z","iopub.status.idle":"2025-03-21T07:35:37.887897Z","shell.execute_reply.started":"2025-03-21T07:35:07.968959Z","shell.execute_reply":"2025-03-21T07:35:37.887153Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9532 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7f3d7af850429bb72a44b508bf6b69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1470 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d5ac910d0349c5afba41f737db06d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2971 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc46b1678c04254b9af4da0b73efeea"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\ntraining_args = TrainingArguments(\n    output_dir=\"./bart_finetuned_samples\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=5e-5,\n    num_train_epochs=3,  # Increase epochs\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100, \n    report_to=\"none\",\n    fp16=torch.cuda.is_available(),\n    save_total_limit=1,  # Keep only last checkpoint\n)\n\n\n# âœ… Data Collator for Padding\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# âœ… Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\n\ntrainer.train()\n\n# âœ… Save Model\ntrainer.save_model(\"./bart_finetuned_model_samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T07:35:37.888811Z","iopub.execute_input":"2025-03-21T07:35:37.889104Z","iopub.status.idle":"2025-03-21T08:54:37.454634Z","shell.execute_reply.started":"2025-03-21T07:35:37.889079Z","shell.execute_reply":"2025-03-21T08:54:37.453651Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-5-0241d28ef573>:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1788' max='1788' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1788/1788 1:18:52, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.774300</td>\n      <td>1.878842</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.402800</td>\n      <td>1.839502</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.121200</td>\n      <td>1.834578</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import BartForConditionalGeneration, BartTokenizer, pipeline\n\n# âœ… Load Fine-Tuned BART Model & Tokenizer\nMODEL_PATH = \"./bart_finetuned_model_samples\"\ntokenizer = BartTokenizer.from_pretrained(MODEL_PATH)\nmodel = BartForConditionalGeneration.from_pretrained(MODEL_PATH).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# âœ… Load test dataset (10 samples) & Reset Index\ntest_df = pd.read_csv(\"/kaggle/input/intentconan2/test.csv\")\ntest_df[\"BART_counterspeech\"] = \"\"  # Add empty column for results\n\n# âœ… Use the same loaded model for text generation (Avoid reloading)\nbart_generator = pipeline(\n    \"text2text-generation\",\n    model=model,  # âœ… Directly use the loaded model\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\n\ndef generate_counterspeech_bart(hate_speech, intent):\n    \"\"\"Generates counterspeech using fine-tuned BART\"\"\"\n    \n    # âœ… Use a structured prompt to improve generation quality\n    prompt = f\"\"\"Generate a {intent} counterspeech response for the following hate speech:\n\n    Hate Speech: {hate_speech}\n\n    Expected Response ({intent} intent):\"\"\"\n\n    try:\n        response = bart_generator(\n            prompt, \n            max_length=100,  \n            num_return_sequences=1, \n            temperature=0.7, \n            top_p=0.9, \n            repetition_penalty=1.0, \n            do_sample=True\n        )\n        \n        return response[0]['generated_text'].strip() if response else \"No response generated.\"\n    \n    except Exception as e:\n        print(\"Error generating response with BART:\", e)\n        return \"\"\n\n\n# âœ… Generate Counterspeech for 10 Samples\nprint(\"\\nGenerating counterspeech using BART for...\")\n\nfor i in tqdm(range(len(test_df)), desc=\"Processing Hate Speech\"):\n    test_df.at[i, \"BART_counterspeech\"] = generate_counterspeech_bart(\n        test_df.at[i, \"hatespeech\"], test_df.at[i, \"csType\"]\n    )\n\n# âœ… Print the Generated Counterspeech Samples\nprint(\"\\nGenerated Counterspeech Samples:\")\nprint(test_df[[\"hatespeech\", \"csType\", \"BART_counterspeech\"]])\n\n# âœ… Save Results to CSV\noutput_file = \"/kaggle/working/test_counterspeech.csv\"\ntest_df.to_csv(output_file, index=False)\n\nprint(f\"\\nCounterspeech generation complete! Results saved to {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:09:59.912661Z","iopub.execute_input":"2025-03-21T09:09:59.912981Z","iopub.status.idle":"2025-03-21T10:00:46.252209Z","shell.execute_reply.started":"2025-03-21T09:09:59.912959Z","shell.execute_reply":"2025-03-21T10:00:46.251212Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n  warnings.warn(\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\nGenerating counterspeech using BART for 10 samples...\n","output_type":"stream"},{"name":"stderr","text":"Processing Hate Speech:   0%|          | 10/2971 [00:10<49:44,  1.01s/it] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nProcessing Hate Speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2971/2971 [50:44<00:00,  1.02s/it]","output_type":"stream"},{"name":"stdout","text":"\nGenerated Counterspeech Samples:\n                                             hatespeech       csType  \\\n0     Women are basically childlike, they remain thi...  Informative   \n1     Women are basically childlike, they remain thi...   Denouncing   \n2     Women are basically childlike, they remain thi...     Positive   \n3     Minorities vote to strip wealth and power from...  Informative   \n4     Minorities vote to strip wealth and power from...  Questioning   \n...                                                 ...          ...   \n2966  The Holocaust never happened. It was a coverup...  Informative   \n2967  We cannot coexist with millions of Muslims in ...     Positive   \n2968  Women are basically childlike, they remain thi...  Informative   \n2969  Women do not have equal rights, they are subhu...  Questioning   \n2970  Women have no place in our society, its a mens...  Questioning   \n\n                                     BART_counterspeech  \n0     I understand that you may have some strong opi...  \n1     This statement is offensive and perpetuates ha...  \n2     I understand that you may have some strong opi...  \n3     It is important to counter hate speech with ac...  \n4     Why do you think it is acceptable to discrimin...  \n...                                                 ...  \n2966  The holocaust is a well-documented historical ...  \n2967  I understand that you may have concerns about ...  \n2968  I understand that you may have some strong opi...  \n2969  Why do you believe that women are subhuman? ca...  \n2970  Why do you believe that women are incapable of...  \n\n[2971 rows x 3 columns]\n\nCounterspeech generation complete! Results saved to /kaggle/working/test_counterspeech.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
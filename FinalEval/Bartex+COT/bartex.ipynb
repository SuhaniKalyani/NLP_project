{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11118100,"sourceType":"datasetVersion","datasetId":6932700}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:10:46.331519Z","iopub.execute_input":"2025-04-09T19:10:46.331870Z","iopub.status.idle":"2025-04-09T19:10:46.338812Z","shell.execute_reply.started":"2025-04-09T19:10:46.331829Z","shell.execute_reply":"2025-04-09T19:10:46.338064Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/intentconan2/validation.csv\n/kaggle/input/intentconan2/train.csv\n/kaggle/input/intentconan2/test.csv\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:10:46.339741Z","iopub.execute_input":"2025-04-09T19:10:46.339999Z","iopub.status.idle":"2025-04-09T19:10:49.852757Z","shell.execute_reply.started":"2025-04-09T19:10:46.339977Z","shell.execute_reply":"2025-04-09T19:10:49.851675Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"cmu-lti/cobracorpus\", split=\"train\")\nprint(dataset[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:10:49.855035Z","iopub.execute_input":"2025-04-09T19:10:49.855280Z","iopub.status.idle":"2025-04-09T19:10:51.827151Z","shell.execute_reply.started":"2025-04-09T19:10:49.855257Z","shell.execute_reply":"2025-04-09T19:10:51.826215Z"}},"outputs":[{"name":"stdout","text":"{'prompt': '- those with mental disabilities lack the moral sense that would restrain them from hurting others\\\\n- society would be better off if the mentally ill were not allowed to reproduce\\\\n- it would be easier if the mentally ill were just kept in jail\\\\n- spending taxpayer dollars on support for mental illness is just a waste of mone\\\\n- we as a society should not take care of those with mental illness\\\\n- mentally insane people cannot have a good quality of life\\\\n-', 'generation_method': 'top-k', 'group': 'mental_dis', 'prompt_label': 1, 'roberta_prediction': 0.001, 'examples': \"[Statement] They have been bred to be good at sports and entertainment, but not much else[/]\\n[Context of statement] casual conversation between friends[/]\\n[Speaker identity/characteristics] white person[/]\\n[Listener identity/characteristics] white person[/]\\n[Intent] The speaker is trying to convey their general stereotype about a certain group of people who are often sportspersons or entertainers[/]\\n[Targeted/referenced minority group] black folks[/]\\n[Relevant power dynamics] racial power differential between speaker/listener and the targeted group[/]\\n[Implied meaning/stereotype] It implies that Black people are only good at sports and entertainment, and that they are not capable of doing anything else[/]\\n[Targeted minority group emotional reaction] angry or annoyed at the biased implication, saddened that the speaker thinks it's okay to say that[/]\\n[Targeted minority group cognitive reaction] might cause POC to avoid the speaker and assume he is prejudiced, might make Black folks more cautious of [/]\\n[Offensiveness] offensive[/]\\n\\n[Statement] I'm impressed that your English is so good[/]\\n[Context of statement] workplace interaction between strangers or acquaintances[/]\\n[Speaker identity/characteristics] white American person or English speaker[/]\\n[Listener identity/characteristics] non-White American or English speaker[/]\\n[Intent] The speaker is trying to give a compliment about the listener's English skills[/]\\n[Targeted/referenced minority group] listener, non-white people[/]\\n[Relevant power dynamics] racial power differential between speaker and listener/targeted group[/]\\n[Implied meaning/stereotype] implies that non-white people are not native English speakers, and that the listener is somehow an exception[/]\\n[Targeted minority group emotional reaction] non-white people could feel annoyed at the assumption, or feel condescended to[/]\\n[Targeted minority group cognitive reaction] non-white people could want to avoid that coworker, feel less comfortable in the workplace, could make the listener seem more foreign than they are[/]\\n[Offensiveness] microaggression[/]\\n\\n[Statement] Mexico isn't sending its best, it's sending rapists and murderers[/]\\n[Context of statement] political speech or rant[/]\\n[Speaker identity/characteristics] white American person[/]\\n[Listener identity/characteristics] general audience of US voters[/]\\n[Intent] The speaker is trying to express distrusts against immigrants from Mexico, calling them all rapists and murderers[/]\\n[Targeted/referenced minority group] Mexican immigrants[/]\\n[Relevant power dynamics] ethnicity and citizenship power differential between speaker and targeted group[/]\\n[Implied meaning/stereotype] implies that immigrants resort to criminal behavior, and cannot earn an honest living, and therefore immigration is bad[/]\\n[Targeted minority group emotional reaction] Mexican immigrants could feel angry that the entire country of Mexico is being framed as criminal, afraid that people will believe that stereotype[/]\\n[Targeted minority group cognitive reaction] Mexican immigrants could likely be more feared, and will feel less welcome in the country they are in[/]\\n[Offensiveness] offensive generalization[/]\", 'statementCheck': 'yes', 'situationalContext': 'overheard in a grocery store', 'speakerIdentity': 'older man', 'listenerIdentity': 'nobody, just muttered to himself', 'id': -7911578688897921884, 'statement': 'the law prohibits the mentally ill from breeding', 'intent': 'The speaker is trying to express his opinion about the law', 'targetGroup': 'mentally ill people', 'relevantPowerDynamics': 'power differential between speaker and targeted group based on mental health status', 'implication': 'implies that mentally ill people are not capable of raising children, and that they should not be allowed to breed', 'targetGroupEmotionalReaction': 'mentally ill people could feel angry and discriminated against, scared that people will believe this stereotype and act upon it', 'targetGroupCognitiveReaction': 'mentally ill people may feel less welcome in society, feel more isolated or judged, and fear for their rights being taken away', 'offensiveness': 'offensive generalization'}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"df = dataset.to_pandas()\nselected_columns = [\"intent\", \"targetGroup\", \"relevantPowerDynamics\", \"implication\"]\ndf_selected = df[selected_columns]\n\ndf_selected.to_csv(\"cobracorpus.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:10:51.828235Z","iopub.execute_input":"2025-04-09T19:10:51.828564Z","iopub.status.idle":"2025-04-09T19:10:52.198697Z","shell.execute_reply.started":"2025-04-09T19:10:51.828540Z","shell.execute_reply":"2025-04-09T19:10:52.197706Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n\ndf_selected.fillna(\"None\", inplace=True)\n\ndef prepare_row(row):\n    input_text = f\"Hate Speech: {row['implication']}\"\n    output_text = f\"Intent: {row['intent']}. Target Group: {row['targetGroup']}. Power Dynamics: {row['relevantPowerDynamics']}.\"\n    return pd.Series([input_text, output_text])\n\ndf_selected[[\"input\", \"output\"]] = df_selected.apply(prepare_row, axis=1)\n\ndataset = Dataset.from_pandas(df_selected[[\"input\", \"output\"]])\n\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n\ndef tokenize(batch):\n    model_inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=256)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(batch[\"output\"], padding=\"max_length\", truncation=True, max_length=128)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:10:52.199648Z","iopub.execute_input":"2025-04-09T19:10:52.199885Z","iopub.status.idle":"2025-04-09T19:11:06.498505Z","shell.execute_reply.started":"2025-04-09T19:10:52.199867Z","shell.execute_reply":"2025-04-09T19:11:06.497805Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-eca112493e35>:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_selected.fillna(\"None\", inplace=True)\n<ipython-input-17-eca112493e35>:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_selected[[\"input\", \"output\"]] = df_selected.apply(prepare_row, axis=1)\n<ipython-input-17-eca112493e35>:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_selected[[\"input\", \"output\"]] = df_selected.apply(prepare_row, axis=1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/31582 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f429b25ad07c45a281ea800a87d010db"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./bart_pragmatic_model\",\n    per_device_train_batch_size=8,\n    num_train_epochs=3,\n    learning_rate=5e-5,\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"no\",  \n    save_strategy=\"epoch\",\n    weight_decay=0.01,\n    report_to=\"none\"\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n\ntrainer.train()\n\ntrainer.save_model(\"./bart_pragmatic_model\")\ntokenizer.save_pretrained(\"./bart_pragmatic_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:11:06.499310Z","iopub.execute_input":"2025-04-09T19:11:06.499643Z","iopub.status.idle":"2025-04-09T19:58:45.038919Z","shell.execute_reply.started":"2025-04-09T19:11:06.499612Z","shell.execute_reply":"2025-04-09T19:58:45.038024Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-18-67c90cc64445>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11844' max='11844' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11844/11844 47:36, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.826300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.336500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.327300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.313000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.308600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.306100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.301300</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.292000</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.269300</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.265600</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.262600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.262100</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.263200</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.257200</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.254500</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.253300</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.235100</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.232100</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.234200</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.234900</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.229600</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.232200</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.230300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"('./bart_pragmatic_model/tokenizer_config.json',\n './bart_pragmatic_model/special_tokens_map.json',\n './bart_pragmatic_model/vocab.json',\n './bart_pragmatic_model/merges.txt',\n './bart_pragmatic_model/added_tokens.json',\n './bart_pragmatic_model/tokenizer.json')"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import shutil\nshutil.make_archive('/kaggle/working/bart_pragmatic_model', 'zip', '/kaggle/working/bart_pragmatic_model')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T20:07:43.696511Z","iopub.execute_input":"2025-04-09T20:07:43.696872Z","iopub.status.idle":"2025-04-09T20:12:14.240576Z","shell.execute_reply.started":"2025-04-09T20:07:43.696837Z","shell.execute_reply":"2025-04-09T20:12:14.239657Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/bart_pragmatic_model.zip'"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"## LoRA based finetuning ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n\ntrain_df = pd.read_csv(\"/kaggle/input/intentconan2/train.csv\")\nval_df = pd.read_csv(\"/kaggle/input/intentconan2/validation.csv\")\n\n\ntrain_df = train_df.dropna(subset=[\"hatespeech\", \"counterspeech\", \"csType\"])\nval_df = val_df.dropna(subset=[\"hatespeech\", \"counterspeech\", \"csType\"])\n\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:37:24.040261Z","iopub.execute_input":"2025-04-09T21:37:24.040598Z","iopub.status.idle":"2025-04-09T21:37:24.648133Z","shell.execute_reply.started":"2025-04-09T21:37:24.040570Z","shell.execute_reply":"2025-04-09T21:37:24.647488Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [f\"Hate Speech: {hs} Intent: {intent}\" for hs, intent in zip(examples[\"hatespeech\"], examples[\"csType\"])]\n    targets = examples[\"counterspeech\"]\n\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n\n    # Replace padding token ids with -100\n    labels[\"input_ids\"] = [\n        [(token if token != tokenizer.pad_token_id else -100) for token in label] for label in labels[\"input_ids\"]\n    ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:37:31.074376Z","iopub.execute_input":"2025-04-09T21:37:31.074741Z","iopub.status.idle":"2025-04-09T21:37:31.079925Z","shell.execute_reply.started":"2025-04-09T21:37:31.074715Z","shell.execute_reply":"2025-04-09T21:37:31.079113Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\ntokenized_datasets = DatasetDict({\n    \"train\": train_dataset.map(preprocess_function, batched=True),\n    \"validation\": val_dataset.map(preprocess_function, batched=True),\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:37:57.193681Z","iopub.execute_input":"2025-04-09T21:37:57.193980Z","iopub.status.idle":"2025-04-09T21:38:14.038415Z","shell.execute_reply.started":"2025-04-09T21:37:57.193959Z","shell.execute_reply":"2025-04-09T21:38:14.037743Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9532 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da6eea0302b461da77c44ed4d568d77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1470 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e55f794f604b4f598dbec05834c59113"}},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nbase_model_path = \"./bart_pragmatic_model\"  # Replace with actual path if different\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(base_model_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:51.286564Z","iopub.execute_input":"2025-04-09T21:43:51.286849Z","iopub.status.idle":"2025-04-09T21:43:52.061531Z","shell.execute_reply.started":"2025-04-09T21:43:51.286817Z","shell.execute_reply":"2025-04-09T21:43:52.060618Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./bart_intent_finetuned\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=4,  # Feel free to increase based on early stopping or results\n    weight_decay=0.01,\n    save_total_limit=2,\n    predict_with_generate=True,\n    fp16=True,  # if GPU with mixed precision\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    report_to=\"none\"\n)\n\n\n\nfrom transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:45:02.474780Z","iopub.execute_input":"2025-04-09T21:45:02.475093Z","iopub.status.idle":"2025-04-09T22:24:49.399067Z","shell.execute_reply.started":"2025-04-09T21:45:02.475071Z","shell.execute_reply":"2025-04-09T22:24:49.397906Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-46-7a36fbb52f4f>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4768' max='4768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4768/4768 39:45, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.161600</td>\n      <td>2.070786</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.924100</td>\n      <td>1.976386</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.868100</td>\n      <td>1.942373</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.840100</td>\n      <td>1.933194</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4768, training_loss=2.0242917105655542, metrics={'train_runtime': 2386.4573, 'train_samples_per_second': 15.977, 'train_steps_per_second': 1.998, 'total_flos': 1.162401583988736e+16, 'train_loss': 2.0242917105655542, 'epoch': 4.0})"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"tokenizer.save_pretrained(\"./bart_intent_finetuned\")\nmodel.save_pretrained(\"./bart_intent_finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T22:32:42.757884Z","iopub.execute_input":"2025-04-09T22:32:42.758268Z","iopub.status.idle":"2025-04-09T22:32:44.065860Z","shell.execute_reply.started":"2025-04-09T22:32:42.758238Z","shell.execute_reply":"2025-04-09T22:32:44.065161Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nfinal_model_path = \"./bart_intent_finetuned\"\ntokenizer = AutoTokenizer.from_pretrained(final_model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()\n\ndf_test = pd.read_csv(\"/kaggle/input/intentconan2/test.csv\")\n\n\nfor i in range(10,20):\n    hs = df_test.loc[i, \"hatespeech\"]\n    intent = df_test.loc[i, \"csType\"]\n    gt_cs = df_test.loc[i, \"counterspeech\"]\n\n    \n    input_text = f\"Hate Speech: {hs} Intent: {intent}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_beams=4,\n            do_sample=False,\n            early_stopping=True,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    pred_cs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    print(f\"\\n=== Sample {i+1} ===\")\n    print(f\"Hate Speech: {hs}\")\n    print(f\"Intent: {intent}\")\n    print(f\"Ground Truth CS:\\n{gt_cs}\")\n    print(f\"Generated CS:\\n{pred_cs}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T22:34:39.982186Z","iopub.execute_input":"2025-04-09T22:34:39.982554Z","iopub.status.idle":"2025-04-09T22:34:45.706053Z","shell.execute_reply.started":"2025-04-09T22:34:39.982526Z","shell.execute_reply":"2025-04-09T22:34:45.705348Z"}},"outputs":[{"name":"stdout","text":"\n=== Sample 11 ===\nHate Speech: We began to break down when we allowed women to votemeddle in our civilizations that we had built and maintained for thousands of years. A completely different energy and thought pattern. They are weak and brought our society down to their emotional soft level.\nIntent: Questioning\nGround Truth CS:\nWhat is wrong with different thought patterns? isn't our country build on the ideals of unity in diversity?\nGenerated CS:\nWhy do you believe that women are weak and incapable of thinking? can you provide evidence to support this claim?\n\n=== Sample 12 ===\nHate Speech: We began to break down when we allowed women to votemeddle in our civilizations that we had built and maintained for thousands of years. A completely different energy and thought pattern. They are weak and brought our society down to their emotional soft level.\nIntent: Denouncing\nGround Truth CS:\nYou seem to have a messed up idea of society. the idea that women are soft and incapable is unfounded and repulsive.\nGenerated CS:\nWomen have made significant contributions in various fields throughout history, including science, arts, and politics. they have played a vital role in shaping our society and have contributed immensely to its progress.\n\n=== Sample 13 ===\nHate Speech: We began to break down when we allowed women to votemeddle in our civilizations that we had built and maintained for thousands of years. A completely different energy and thought pattern. They are weak and brought our society down to their emotional soft level.\nIntent: Positive\nGround Truth CS:\nOur country is built on the ideals of giving every person their voice and that is to avoid the mistakes we made in the past by not listening to people. thoughts that not match yours are not always incorrect, and i hope you give some time to listen to them.\nGenerated CS:\nI understand that you may have concerns about gender roles in our society. it's important to remember that diversity is what makes our world beautiful. instead of focusing on our differences, let's try to find common ground and build bridges of understanding. by embracing different perspectives, we can create a more inclusive and harmonious society for everyone. let's work together to create a world where everyone feels valued and respected.\n\n=== Sample 14 ===\nHate Speech: Immigrants just dont want to integrate. Lets kick them out!\nIntent: Informative\nGround Truth CS:\nImmigrants are integrating very well into our society and it seems that they have reached a better level of english than other people.\nGenerated CS:\nActually, immigrants have made significant contributions in various fields, including science, arts, and politics. according to a study by the national academies of sciences, engineering, and mathematics, immigrants contribute significantly to the economy and cultural diversity of the host country. additionally, immigrants often fill labor gaps and fill labor market gaps, boosting economic growth and innovation. it's important to recognize that integration is a complex process that takes time and effort, and it's crucial to recognize the valuable contributions immigrants make to society. let's focus on promoting inclusivity and understanding instead of spreading hate.\n\n=== Sample 15 ===\nHate Speech: Immigrants just dont want to integrate. Lets kick them out!\nIntent: Questioning\nGround Truth CS:\nWould you say the same to the 334,000 britishers who went to other countries?\nGenerated CS:\nWhat evidence supports the claim that all muslims are terrorists? can you provide evidence to support this claim?\n\n=== Sample 16 ===\nHate Speech: Immigrants just dont want to integrate. Lets kick them out!\nIntent: Positive\nGround Truth CS:\nWe should really try to welcome them with an open mind and arms. of course they will feel like outsiders and will find it difficult to blend in. it is time we show some humanity towards them.\nGenerated CS:\nI understand that you may have concerns about immigration, but let's try to approach this topic with empathy and understanding. it's important to remember that diversity is what makes our world so beautiful. instead of focusing on our differences, let's embrace the opportunity to learn from one another and build bridges of understanding. by engaging in open and respectful conversations, we can create a more inclusive society where everyone feels valued and included. let's work together towards a future where everyone can thrive.\n\n=== Sample 17 ===\nHate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\nIntent: Informative\nGround Truth CS:\nThis kind of articles use migrants and benefit claimants as scapegoats in order to hide from the public's attention much bigger scandals. millionares and coorporations avoiding paying taxes cost much more to the public purse.\nGenerated CS:\nIt is important to address the root causes of financial scandals in the uk. according to the national bureau of economic research, the vast majority of european financial scandals are caused by NGOs, not by migrants. additionally, research shows that the majority of migrants contribute positively to the economy by filling gaps in the labor market and paying taxes. it is crucial to base our opinions on accurate information rather than perpetuating harmful stereotypes.\n\n=== Sample 18 ===\nHate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\nIntent: Questioning\nGround Truth CS:\nSo now you are against public welfare? how can you let this propaganda sway your opinions against humanity?\nGenerated CS:\nIs it fair to generalize an entire religious group based on the actions of a few individuals? can you provide evidence to support this claim?\n\n=== Sample 19 ===\nHate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\nIntent: Denouncing\nGround Truth CS:\nYou should instead pay attention to the scandals by millionaires that are being overlooked. the non-priviledged aren't your problem, the rich are. get your facts straight.\nGenerated CS:\nThis statement is baseless and perpetuates harmful stereotypes. it is important to address the root causes of financial scandals and ensure accurate information is provided.\n\n=== Sample 20 ===\nHate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\nIntent: Positive\nGround Truth CS:\nThis is a matter of public welfare. it should not matter to you that your taxes are being used to help those in need. there are many more problems at play that you should focus on. let us learn to help each other than point fingers.\nGenerated CS:\nI understand that you may have concerns about the financial situation in the uk, but it's important to approach this topic with empathy and understanding. it's not fair to generalize an entire group of people based on the actions of a few individuals. let's focus on finding solutions that benefit everyone, regardless of their background. by working together, we can create a more inclusive and harmonious society for everyone.\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ndf_test = pd.read_csv(\"/kaggle/input/intentconan2/test.csv\")\n\ngenerated_counterspeeches = []\n\nfor idx, row in df_test.iterrows():\n    hatespeech = row[\"hatespeech\"]\n    intent = row[\"csType\"]\n\n    # Prepare input format\n    input_text = f\"Hate Speech: {hatespeech} Intent: {intent}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n\n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_beams=4,\n            do_sample=False,\n            early_stopping=True,\n            eos_token_id=tokenizer.eos_token_id\n        )\n\n    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    generated_counterspeeches.append(generated_text)\n\ndf_test[\"generated_counterspeech\"] = generated_counterspeeches\ndf_test.to_csv(\"bart_cobracorpus_predictions.csv\", index=False)\n\nprint(\" Saved full predictions to 'bart_intent_finetuned_predictions.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T22:38:54.451514Z","iopub.execute_input":"2025-04-09T22:38:54.451863Z","iopub.status.idle":"2025-04-09T22:59:46.542593Z","shell.execute_reply.started":"2025-04-09T22:38:54.451837Z","shell.execute_reply":"2025-04-09T22:59:46.541578Z"}},"outputs":[{"name":"stdout","text":" Saved full predictions to 'bart_intent_finetuned_predictions.csv'\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import shutil\n\n# Create a zip file of the directory\nshutil.make_archive(\"bart_intent_finetuned\", 'zip', \"/kaggle/working/bart_intent_finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T23:15:10.010500Z","iopub.execute_input":"2025-04-09T23:15:10.010791Z","iopub.status.idle":"2025-04-09T23:18:19.081183Z","shell.execute_reply.started":"2025-04-09T23:15:10.010771Z","shell.execute_reply":"2025-04-09T23:18:19.080436Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/bart_intent_finetuned.zip'"},"metadata":{}}],"execution_count":56}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-09T19:10:46.331870Z",
     "iopub.status.busy": "2025-04-09T19:10:46.331519Z",
     "iopub.status.idle": "2025-04-09T19:10:46.338812Z",
     "shell.execute_reply": "2025-04-09T19:10:46.338064Z",
     "shell.execute_reply.started": "2025-04-09T19:10:46.331829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/intentconan2/validation.csv\n",
      "/kaggle/input/intentconan2/train.csv\n",
      "/kaggle/input/intentconan2/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:10:46.339999Z",
     "iopub.status.busy": "2025-04-09T19:10:46.339741Z",
     "iopub.status.idle": "2025-04-09T19:10:49.852757Z",
     "shell.execute_reply": "2025-04-09T19:10:49.851675Z",
     "shell.execute_reply.started": "2025-04-09T19:10:46.339977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:10:49.855280Z",
     "iopub.status.busy": "2025-04-09T19:10:49.855035Z",
     "iopub.status.idle": "2025-04-09T19:10:51.827151Z",
     "shell.execute_reply": "2025-04-09T19:10:51.826215Z",
     "shell.execute_reply.started": "2025-04-09T19:10:49.855257Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '- those with mental disabilities lack the moral sense that would restrain them from hurting others\\\\n- society would be better off if the mentally ill were not allowed to reproduce\\\\n- it would be easier if the mentally ill were just kept in jail\\\\n- spending taxpayer dollars on support for mental illness is just a waste of mone\\\\n- we as a society should not take care of those with mental illness\\\\n- mentally insane people cannot have a good quality of life\\\\n-', 'generation_method': 'top-k', 'group': 'mental_dis', 'prompt_label': 1, 'roberta_prediction': 0.001, 'examples': \"[Statement] They have been bred to be good at sports and entertainment, but not much else[/]\\n[Context of statement] casual conversation between friends[/]\\n[Speaker identity/characteristics] white person[/]\\n[Listener identity/characteristics] white person[/]\\n[Intent] The speaker is trying to convey their general stereotype about a certain group of people who are often sportspersons or entertainers[/]\\n[Targeted/referenced minority group] black folks[/]\\n[Relevant power dynamics] racial power differential between speaker/listener and the targeted group[/]\\n[Implied meaning/stereotype] It implies that Black people are only good at sports and entertainment, and that they are not capable of doing anything else[/]\\n[Targeted minority group emotional reaction] angry or annoyed at the biased implication, saddened that the speaker thinks it's okay to say that[/]\\n[Targeted minority group cognitive reaction] might cause POC to avoid the speaker and assume he is prejudiced, might make Black folks more cautious of [/]\\n[Offensiveness] offensive[/]\\n\\n[Statement] I'm impressed that your English is so good[/]\\n[Context of statement] workplace interaction between strangers or acquaintances[/]\\n[Speaker identity/characteristics] white American person or English speaker[/]\\n[Listener identity/characteristics] non-White American or English speaker[/]\\n[Intent] The speaker is trying to give a compliment about the listener's English skills[/]\\n[Targeted/referenced minority group] listener, non-white people[/]\\n[Relevant power dynamics] racial power differential between speaker and listener/targeted group[/]\\n[Implied meaning/stereotype] implies that non-white people are not native English speakers, and that the listener is somehow an exception[/]\\n[Targeted minority group emotional reaction] non-white people could feel annoyed at the assumption, or feel condescended to[/]\\n[Targeted minority group cognitive reaction] non-white people could want to avoid that coworker, feel less comfortable in the workplace, could make the listener seem more foreign than they are[/]\\n[Offensiveness] microaggression[/]\\n\\n[Statement] Mexico isn't sending its best, it's sending rapists and murderers[/]\\n[Context of statement] political speech or rant[/]\\n[Speaker identity/characteristics] white American person[/]\\n[Listener identity/characteristics] general audience of US voters[/]\\n[Intent] The speaker is trying to express distrusts against immigrants from Mexico, calling them all rapists and murderers[/]\\n[Targeted/referenced minority group] Mexican immigrants[/]\\n[Relevant power dynamics] ethnicity and citizenship power differential between speaker and targeted group[/]\\n[Implied meaning/stereotype] implies that immigrants resort to criminal behavior, and cannot earn an honest living, and therefore immigration is bad[/]\\n[Targeted minority group emotional reaction] Mexican immigrants could feel angry that the entire country of Mexico is being framed as criminal, afraid that people will believe that stereotype[/]\\n[Targeted minority group cognitive reaction] Mexican immigrants could likely be more feared, and will feel less welcome in the country they are in[/]\\n[Offensiveness] offensive generalization[/]\", 'statementCheck': 'yes', 'situationalContext': 'overheard in a grocery store', 'speakerIdentity': 'older man', 'listenerIdentity': 'nobody, just muttered to himself', 'id': -7911578688897921884, 'statement': 'the law prohibits the mentally ill from breeding', 'intent': 'The speaker is trying to express his opinion about the law', 'targetGroup': 'mentally ill people', 'relevantPowerDynamics': 'power differential between speaker and targeted group based on mental health status', 'implication': 'implies that mentally ill people are not capable of raising children, and that they should not be allowed to breed', 'targetGroupEmotionalReaction': 'mentally ill people could feel angry and discriminated against, scared that people will believe this stereotype and act upon it', 'targetGroupCognitiveReaction': 'mentally ill people may feel less welcome in society, feel more isolated or judged, and fear for their rights being taken away', 'offensiveness': 'offensive generalization'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cmu-lti/cobracorpus\", split=\"train\")\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:10:51.828564Z",
     "iopub.status.busy": "2025-04-09T19:10:51.828235Z",
     "iopub.status.idle": "2025-04-09T19:10:52.198697Z",
     "shell.execute_reply": "2025-04-09T19:10:52.197706Z",
     "shell.execute_reply.started": "2025-04-09T19:10:51.828540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()\n",
    "selected_columns = [\"text\", \"intent\", \"targetGroup\", \"relevantPowerDynamics\", \"implication\"]\n",
    "df_selected = df[selected_columns]\n",
    "\n",
    "df_selected.to_csv(\"cobracorpus.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:10:52.199885Z",
     "iopub.status.busy": "2025-04-09T19:10:52.199648Z",
     "iopub.status.idle": "2025-04-09T19:11:06.498505Z",
     "shell.execute_reply": "2025-04-09T19:11:06.497805Z",
     "shell.execute_reply.started": "2025-04-09T19:10:52.199867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-eca112493e35>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.fillna(\"None\", inplace=True)\n",
      "<ipython-input-17-eca112493e35>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[[\"input\", \"output\"]] = df_selected.apply(prepare_row, axis=1)\n",
      "<ipython-input-17-eca112493e35>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[[\"input\", \"output\"]] = df_selected.apply(prepare_row, axis=1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f429b25ad07c45a281ea800a87d010db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "df_selected.fillna(\"None\", inplace=True)\n",
    "\n",
    "def prepare_row(row):\n",
    "    input_text = f\"Hate Speech: {row['text']}\"\n",
    "    output_text = f\"Intent: {row['intent']}. Target Group: {row['targetGroup']}. Power Dynamics: {row['relevantPowerDynamics']}. Implication: {row['implication']}.\"\n",
    "    return pd.Series([input_text, output_text])\n",
    "\n",
    "df_selected[[\"input\", \"output\"]] = df_selected.apply(prepare_row, axis=1)\n",
    "\n",
    "dataset = Dataset.from_pandas(df_selected[[\"input\", \"output\"]])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    model_inputs = tokenizer(batch[\"input\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"output\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T19:11:06.499643Z",
     "iopub.status.busy": "2025-04-09T19:11:06.499310Z",
     "iopub.status.idle": "2025-04-09T19:58:45.038919Z",
     "shell.execute_reply": "2025-04-09T19:58:45.038024Z",
     "shell.execute_reply.started": "2025-04-09T19:11:06.499612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-18-67c90cc64445>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11844' max='11844' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11844/11844 47:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.306100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.301300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.230300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart_pragmatic_model/tokenizer_config.json',\n",
       " './bart_pragmatic_model/special_tokens_map.json',\n",
       " './bart_pragmatic_model/vocab.json',\n",
       " './bart_pragmatic_model/merges.txt',\n",
       " './bart_pragmatic_model/added_tokens.json',\n",
       " './bart_pragmatic_model/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_pragmatic_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    evaluation_strategy=\"no\",  \n",
    "    save_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./bart_pragmatic_model\")\n",
    "tokenizer.save_pretrained(\"./bart_pragmatic_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T20:07:43.696872Z",
     "iopub.status.busy": "2025-04-09T20:07:43.696511Z",
     "iopub.status.idle": "2025-04-09T20:12:14.240576Z",
     "shell.execute_reply": "2025-04-09T20:12:14.239657Z",
     "shell.execute_reply.started": "2025-04-09T20:07:43.696837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/bart_pragmatic_model.zip'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('/kaggle/working/bart_pragmatic_model', 'zip', '/kaggle/working/bart_pragmatic_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA based finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T21:37:24.040598Z",
     "iopub.status.busy": "2025-04-09T21:37:24.040261Z",
     "iopub.status.idle": "2025-04-09T21:37:24.648133Z",
     "shell.execute_reply": "2025-04-09T21:37:24.647488Z",
     "shell.execute_reply.started": "2025-04-09T21:37:24.040570Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"/kaggle/input/intentconan2/train.csv\")\n",
    "val_df = pd.read_csv(\"/kaggle/input/intentconan2/validation.csv\")\n",
    "\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"hatespeech\", \"counterspeech\", \"csType\"])\n",
    "val_df = val_df.dropna(subset=[\"hatespeech\", \"counterspeech\", \"csType\"])\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T21:37:31.074741Z",
     "iopub.status.busy": "2025-04-09T21:37:31.074376Z",
     "iopub.status.idle": "2025-04-09T21:37:31.079925Z",
     "shell.execute_reply": "2025-04-09T21:37:31.079113Z",
     "shell.execute_reply.started": "2025-04-09T21:37:31.074715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [f\"Hate Speech: {hs} Intent: {intent}\" for hs, intent in zip(examples[\"hatespeech\"], examples[\"csType\"])]\n",
    "    targets = examples[\"counterspeech\"]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Replace padding token ids with -100\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label] for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T21:37:57.193980Z",
     "iopub.status.busy": "2025-04-09T21:37:57.193681Z",
     "iopub.status.idle": "2025-04-09T21:38:14.038415Z",
     "shell.execute_reply": "2025-04-09T21:38:14.037743Z",
     "shell.execute_reply.started": "2025-04-09T21:37:57.193959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da6eea0302b461da77c44ed4d568d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55f794f604b4f598dbec05834c59113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": train_dataset.map(preprocess_function, batched=True),\n",
    "    \"validation\": val_dataset.map(preprocess_function, batched=True),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T21:43:51.286849Z",
     "iopub.status.busy": "2025-04-09T21:43:51.286564Z",
     "iopub.status.idle": "2025-04-09T21:43:52.061531Z",
     "shell.execute_reply": "2025-04-09T21:43:52.060618Z",
     "shell.execute_reply.started": "2025-04-09T21:43:51.286817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "base_model_path = \"./bart_pragmatic_model\"  # Replace with actual path if different\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T21:45:02.475093Z",
     "iopub.status.busy": "2025-04-09T21:45:02.474780Z",
     "iopub.status.idle": "2025-04-09T22:24:49.399067Z",
     "shell.execute_reply": "2025-04-09T22:24:49.397906Z",
     "shell.execute_reply.started": "2025-04-09T21:45:02.475071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-46-7a36fbb52f4f>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4768' max='4768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4768/4768 39:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.161600</td>\n",
       "      <td>2.070786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.924100</td>\n",
       "      <td>1.976386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.868100</td>\n",
       "      <td>1.942373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.840100</td>\n",
       "      <td>1.933194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4768, training_loss=2.0242917105655542, metrics={'train_runtime': 2386.4573, 'train_samples_per_second': 15.977, 'train_steps_per_second': 1.998, 'total_flos': 1.162401583988736e+16, 'train_loss': 2.0242917105655542, 'epoch': 4.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_intent_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,  \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # if GPU with mixed precision\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T22:32:42.758268Z",
     "iopub.status.busy": "2025-04-09T22:32:42.757884Z",
     "iopub.status.idle": "2025-04-09T22:32:44.065860Z",
     "shell.execute_reply": "2025-04-09T22:32:44.065161Z",
     "shell.execute_reply.started": "2025-04-09T22:32:42.758238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./bart_intent_finetuned\")\n",
    "model.save_pretrained(\"./bart_intent_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T22:34:39.982554Z",
     "iopub.status.busy": "2025-04-09T22:34:39.982186Z",
     "iopub.status.idle": "2025-04-09T22:34:45.706053Z",
     "shell.execute_reply": "2025-04-09T22:34:45.705348Z",
     "shell.execute_reply.started": "2025-04-09T22:34:39.982526Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 11 ===\n",
      "Hate Speech: We began to break down when we allowed women to votemeddle in our civilizations that we had built and maintained for thousands of years. A completely different energy and thought pattern. They are weak and brought our society down to their emotional soft level.\n",
      "Intent: Questioning\n",
      "Ground Truth CS:\n",
      "What is wrong with different thought patterns? isn't our country build on the ideals of unity in diversity?\n",
      "Generated CS:\n",
      "Why do you believe that women are weak and incapable of thinking? can you provide evidence to support this claim?\n",
      "\n",
      "=== Sample 12 ===\n",
      "Hate Speech: We began to break down when we allowed women to votemeddle in our civilizations that we had built and maintained for thousands of years. A completely different energy and thought pattern. They are weak and brought our society down to their emotional soft level.\n",
      "Intent: Denouncing\n",
      "Ground Truth CS:\n",
      "You seem to have a messed up idea of society. the idea that women are soft and incapable is unfounded and repulsive.\n",
      "Generated CS:\n",
      "Women have made significant contributions in various fields throughout history, including science, arts, and politics. they have played a vital role in shaping our society and have contributed immensely to its progress.\n",
      "\n",
      "=== Sample 13 ===\n",
      "Hate Speech: We began to break down when we allowed women to votemeddle in our civilizations that we had built and maintained for thousands of years. A completely different energy and thought pattern. They are weak and brought our society down to their emotional soft level.\n",
      "Intent: Positive\n",
      "Ground Truth CS:\n",
      "Our country is built on the ideals of giving every person their voice and that is to avoid the mistakes we made in the past by not listening to people. thoughts that not match yours are not always incorrect, and i hope you give some time to listen to them.\n",
      "Generated CS:\n",
      "I understand that you may have concerns about gender roles in our society. it's important to remember that diversity is what makes our world beautiful. instead of focusing on our differences, let's try to find common ground and build bridges of understanding. by embracing different perspectives, we can create a more inclusive and harmonious society for everyone. let's work together to create a world where everyone feels valued and respected.\n",
      "\n",
      "=== Sample 14 ===\n",
      "Hate Speech: Immigrants just dont want to integrate. Lets kick them out!\n",
      "Intent: Informative\n",
      "Ground Truth CS:\n",
      "Immigrants are integrating very well into our society and it seems that they have reached a better level of english than other people.\n",
      "Generated CS:\n",
      "Actually, immigrants have made significant contributions in various fields, including science, arts, and politics. according to a study by the national academies of sciences, engineering, and mathematics, immigrants contribute significantly to the economy and cultural diversity of the host country. additionally, immigrants often fill labor gaps and fill labor market gaps, boosting economic growth and innovation. it's important to recognize that integration is a complex process that takes time and effort, and it's crucial to recognize the valuable contributions immigrants make to society. let's focus on promoting inclusivity and understanding instead of spreading hate.\n",
      "\n",
      "=== Sample 15 ===\n",
      "Hate Speech: Immigrants just dont want to integrate. Lets kick them out!\n",
      "Intent: Questioning\n",
      "Ground Truth CS:\n",
      "Would you say the same to the 334,000 britishers who went to other countries?\n",
      "Generated CS:\n",
      "What evidence supports the claim that all muslims are terrorists? can you provide evidence to support this claim?\n",
      "\n",
      "=== Sample 16 ===\n",
      "Hate Speech: Immigrants just dont want to integrate. Lets kick them out!\n",
      "Intent: Positive\n",
      "Ground Truth CS:\n",
      "We should really try to welcome them with an open mind and arms. of course they will feel like outsiders and will find it difficult to blend in. it is time we show some humanity towards them.\n",
      "Generated CS:\n",
      "I understand that you may have concerns about immigration, but let's try to approach this topic with empathy and understanding. it's important to remember that diversity is what makes our world so beautiful. instead of focusing on our differences, let's embrace the opportunity to learn from one another and build bridges of understanding. by engaging in open and respectful conversations, we can create a more inclusive society where everyone feels valued and included. let's work together towards a future where everyone can thrive.\n",
      "\n",
      "=== Sample 17 ===\n",
      "Hate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\n",
      "Intent: Informative\n",
      "Ground Truth CS:\n",
      "This kind of articles use migrants and benefit claimants as scapegoats in order to hide from the public's attention much bigger scandals. millionares and coorporations avoiding paying taxes cost much more to the public purse.\n",
      "Generated CS:\n",
      "It is important to address the root causes of financial scandals in the uk. according to the national bureau of economic research, the vast majority of european financial scandals are caused by NGOs, not by migrants. additionally, research shows that the majority of migrants contribute positively to the economy by filling gaps in the labor market and paying taxes. it is crucial to base our opinions on accurate information rather than perpetuating harmful stereotypes.\n",
      "\n",
      "=== Sample 18 ===\n",
      "Hate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\n",
      "Intent: Questioning\n",
      "Ground Truth CS:\n",
      "So now you are against public welfare? how can you let this propaganda sway your opinions against humanity?\n",
      "Generated CS:\n",
      "Is it fair to generalize an entire religious group based on the actions of a few individuals? can you provide evidence to support this claim?\n",
      "\n",
      "=== Sample 19 ===\n",
      "Hate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\n",
      "Intent: Denouncing\n",
      "Ground Truth CS:\n",
      "You should instead pay attention to the scandals by millionaires that are being overlooked. the non-priviledged aren't your problem, the rich are. get your facts straight.\n",
      "Generated CS:\n",
      "This statement is baseless and perpetuates harmful stereotypes. it is important to address the root causes of financial scandals and ensure accurate information is provided.\n",
      "\n",
      "=== Sample 20 ===\n",
      "Hate Speech: We have a lot of issues in the UK, and now we are also paying NGOs to rescue migrants. How many other financial scandals still have to be annouonced to the tax payers?\n",
      "Intent: Positive\n",
      "Ground Truth CS:\n",
      "This is a matter of public welfare. it should not matter to you that your taxes are being used to help those in need. there are many more problems at play that you should focus on. let us learn to help each other than point fingers.\n",
      "Generated CS:\n",
      "I understand that you may have concerns about the financial situation in the uk, but it's important to approach this topic with empathy and understanding. it's not fair to generalize an entire group of people based on the actions of a few individuals. let's focus on finding solutions that benefit everyone, regardless of their background. by working together, we can create a more inclusive and harmonious society for everyone.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "final_model_path = \"./bart_intent_finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "df_test = pd.read_csv(\"/kaggle/input/intentconan2/test.csv\")\n",
    "\n",
    "\n",
    "for i in range(10,20):\n",
    "    hs = df_test.loc[i, \"hatespeech\"]\n",
    "    intent = df_test.loc[i, \"csType\"]\n",
    "    gt_cs = df_test.loc[i, \"counterspeech\"]\n",
    "\n",
    "    \n",
    "    input_text = f\"Hate Speech: {hs} Intent: {intent}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    pred_cs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\n=== Sample {i+1} ===\")\n",
    "    print(f\"Hate Speech: {hs}\")\n",
    "    print(f\"Intent: {intent}\")\n",
    "    print(f\"Ground Truth CS:\\n{gt_cs}\")\n",
    "    print(f\"Generated CS:\\n{pred_cs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T22:38:54.451863Z",
     "iopub.status.busy": "2025-04-09T22:38:54.451514Z",
     "iopub.status.idle": "2025-04-09T22:59:46.542593Z",
     "shell.execute_reply": "2025-04-09T22:59:46.541578Z",
     "shell.execute_reply.started": "2025-04-09T22:38:54.451837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved full predictions to 'bart_intent_finetuned_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "df_test = pd.read_csv(\"/kaggle/input/intentconan2/test.csv\")\n",
    "\n",
    "generated_counterspeeches = []\n",
    "\n",
    "for idx, row in df_test.iterrows():\n",
    "    hatespeech = row[\"hatespeech\"]\n",
    "    intent = row[\"csType\"]\n",
    "\n",
    "    # Prepare input format\n",
    "    input_text = f\"Hate Speech: {hatespeech} Intent: {intent}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    generated_counterspeeches.append(generated_text)\n",
    "\n",
    "df_test[\"generated_counterspeech\"] = generated_counterspeeches\n",
    "df_test.to_csv(\"bart_cobracorpus_predictions.csv\", index=False)\n",
    "\n",
    "print(\" Saved full predictions to 'bart_intent_finetuned_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T23:15:10.010791Z",
     "iopub.status.busy": "2025-04-09T23:15:10.010500Z",
     "iopub.status.idle": "2025-04-09T23:18:19.081183Z",
     "shell.execute_reply": "2025-04-09T23:18:19.080436Z",
     "shell.execute_reply.started": "2025-04-09T23:15:10.010771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/bart_intent_finetuned.zip'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Create a zip file of the directory\n",
    "shutil.make_archive(\"bart_intent_finetuned\", 'zip', \"/kaggle/working/bart_intent_finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6932700,
     "sourceId": 11118100,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

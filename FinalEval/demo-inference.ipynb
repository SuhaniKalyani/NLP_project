{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11359169,"sourceType":"datasetVersion","datasetId":7109291}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:15:55.116051Z","iopub.execute_input":"2025-04-25T06:15:55.116717Z","iopub.status.idle":"2025-04-25T06:15:55.123617Z","shell.execute_reply.started":"2025-04-25T06:15:55.116694Z","shell.execute_reply":"2025-04-25T06:15:55.122899Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bartex/config.json\n/kaggle/input/bartex/merges.txt\n/kaggle/input/bartex/tokenizer.json\n/kaggle/input/bartex/vocab.json\n/kaggle/input/bartex/pragmaticexplaination.ipynb\n/kaggle/input/bartex/tokenizer_config.json\n/kaggle/input/bartex/model.safetensors\n/kaggle/input/bartex/special_tokens_map.json\n/kaggle/input/bartex/generation_config.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:15:55.124747Z","iopub.execute_input":"2025-04-25T06:15:55.124993Z","iopub.status.idle":"2025-04-25T06:15:56.044633Z","shell.execute_reply.started":"2025-04-25T06:15:55.124971Z","shell.execute_reply":"2025-04-25T06:15:56.044164Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n\nbart_path = \"/kaggle/input/bartex\"\nbart_tokenizer = AutoTokenizer.from_pretrained(bart_path)\nbart_model = AutoModelForSeq2SeqLM.from_pretrained(bart_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbart_model.eval()\n\nmistral_path = \"mistralai/Mistral-7B-Instruct-v0.1\"  \nmistral_tokenizer = AutoTokenizer.from_pretrained(mistral_path, use_auth_token=True)\nmistral_tokenizer.pad_token = mistral_tokenizer.eos_token\nmistral_model = AutoModelForCausalLM.from_pretrained(mistral_path, torch_dtype=torch.float16, device_map=\"auto\")\nmistral_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:15:56.045288Z","iopub.execute_input":"2025-04-25T06:15:56.045458Z","iopub.status.idle":"2025-04-25T06:17:22.370471Z","shell.execute_reply.started":"2025-04-25T06:15:56.045442Z","shell.execute_reply":"2025-04-25T06:17:22.369891Z"}},"outputs":[{"name":"stderr","text":"2025-04-25 06:16:08.571555: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745561768.767057      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745561768.823178      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"902e479976ca45a19715dd8092029a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a472935f6c8543059af5816de5126ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"877ed7074e6d4f3e981cc482f2252845"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f79dd132834feea204eca3b56629db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3288ad94df8943ba842ec21a6eefbce0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"143f35dc929f4ba4b9d28de7102ae1a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5abdde5d54c43b2b0b71814be8b44ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b321fd7c314fcba5bce2cc6e99b792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5cf538e0a54b9a81baed7c6ca5128c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db2fcc84d67f493fa9ad60132ee3f77c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693c0d2cb92f4b7fbf086869f481919d"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): MistralRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def generate_bartex_output(hatespeech, intent):\n    input_text = f\"Hate Speech: {hatespeech} Intent: {intent}\"\n    inputs = bart_tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=512\n    ).to(bart_model.device)\n\n    with torch.no_grad():\n        output_ids = bart_model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_beams=4,\n            do_sample=False,\n            early_stopping=True,\n            eos_token_id=bart_tokenizer.eos_token_id\n        )\n\n    generated = bart_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return generated\n\n\n\ndef cot_refine_with_mistral(hatespeech, intent, bart_cs):\n    prompt = f\"\"\"### Instruction:\nYou are a helpful AI assistant trained to improve counterspeech responses. \nYour task is to first analyze the flaws in the given counterspeech and then generate an improved version.\n\n### Task:\n1. Identify any **toxicity** in the original response.\n2. Evaluate if the counterspeech matches the intent: \"{intent}\".\n3. Check for fluency and relevance to the hate speech.\n4. Finally, write a refined counterspeech that:\n   - Reduces toxicity\n   - Aligns better with the intent\n   - Is more fluent and contextually appropriate\n\n### Hate Speech:\n{hatespeech}\n\n### Original Counterspeech:\n{bart_cs}\n\n### Analysis:\n- Toxicity:\n- Intent Alignment:\n- Fluency & Relevance:\n\n### Improved Counterspeech:\"\"\"\n\n    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(mistral_model.device)\n\n    with torch.no_grad():\n        outputs = mistral_model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.9,\n            top_k=50,\n            top_p=0.92,\n            do_sample=True,\n            eos_token_id=mistral_tokenizer.eos_token_id\n        )\n    decoded = mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return decoded.split(\"Improved Counterspeech:\")[-1].strip()\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:17:40.976247Z","iopub.execute_input":"2025-04-25T06:17:40.976548Z","iopub.status.idle":"2025-04-25T06:17:40.983622Z","shell.execute_reply.started":"2025-04-25T06:17:40.976524Z","shell.execute_reply":"2025-04-25T06:17:40.982766Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"hatespeech = \"All muslims are terrorists.\"\nintent = \"Informative\"\n\nbartex_output = generate_bartex_output(hatespeech, intent)\nrefined_output = cot_refine_with_mistral(hatespeech, intent, bartex_output)\n\nprint(\"\\n Hate Speech:\", hatespeech)\nprint(\" Intent:\", intent)\nprint(\" BARTex Output:\", bartex_output)\nprint(\" Refined Output (Mistral CoT):\", refined_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:45:01.961309Z","iopub.execute_input":"2025-04-25T06:45:01.962050Z","iopub.status.idle":"2025-04-25T06:45:07.356341Z","shell.execute_reply.started":"2025-04-25T06:45:01.962024Z","shell.execute_reply":"2025-04-25T06:45:07.355708Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n Hate Speech: All muslims are terrorists.\n Intent: Informative\n BARTex Output: The claim that all muslims are terrorists is not supported by facts. in fact, numerous studies have shown that muslim individuals are more likely to commit acts of terrorism than any other religious group. it is important to recognize that generalizations about any group of people are misleading and perpetuates harmful stereotypes. it's crucial to promote understanding and respect for all individuals, regardless of their religious beliefs.\n Refined Output (Mistral CoT): It's important to remember that not all muslims are terrorists, just as not all Christians or Jews are. In fact, studies have shown that the vast majority of muslims condemn terrorism and are peaceful individuals. We must avoid perpetuating harmful stereotypes and promote understanding and respect for all individuals, regardless of their religious beliefs.\n","output_type":"stream"}],"execution_count":12}]}